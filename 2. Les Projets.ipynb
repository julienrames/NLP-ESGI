{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Les Projets\n",
    "\n",
    "* POS Tagging (en ou fr)\n",
    "* Reconnaissance d'entités nommées \n",
    "* Catégorisation des commentaires (Amazon)\n",
    "\n",
    "** Objectifs: **\n",
    "Implémenter les deux \"pipelines\" d'un système TALN, ou en general d'un system Machine Learning.\n",
    "* Pipeline d'Entraînement\n",
    "    * Entraînement\n",
    "    * Testing\n",
    "* Pipeline de Production\n",
    "    * Analysis as a service (REST API ou Broker de messages)\n",
    "    * Présentation des résultats d'analyse. Deux formats: json (résultats purs) et html (résultats marqués sur le texte original)\n",
    "    \n",
    "** Base d'évaluation **\n",
    "* L'Entraînement a fourni un modèle avec un score f1 supérieur à 50% (Évaluation sur données \"testing\").\n",
    "* Test driven development\n",
    "* Qualité du code (référence: Matériel du 1er cour, [Hitchhiker's guide](http://docs.python-guide.org/en/latest/writing/style/), [Code Like a Pythonista](http://python.net/~goodger/projects/pycon/2007/idiomatic/handout.html))\n",
    "* Commit sur github à la fin de chaque cours. Presentation du project à la fin.\n",
    "\n",
    "** Bonus **\n",
    "* Utilisation des design patterns, e.g. factory, iterator, etc.\n",
    "* Niveau de codage (e.g. utilisation de python avancé, organisation du code, logging au lieu de print, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Vue d'ensemble\n",
    "\n",
    "### Pipeline d'Entraînement\n",
    "\n",
    "![NLP](pictures/Module%204/Training%20pipline.PNG)\n",
    "\n",
    "\n",
    "### Pipeline de Production\n",
    "\n",
    "\n",
    "![NLP](pictures/Module%204/Production%20pipeline.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Étiquetage morpho-syntaxique (POS tagging)\n",
    "![NLP](pictures/POS%20tagging.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exemple avec NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moby Dick sample: The pale Usher--threadbare in coat, heart, body, and brain; I see him\r\n",
      "now.  He was ever dusting his old lexicons and grammars, with a queer\r\n",
      "handkerchief, mockingly embellished with \n",
      "('The', 'DT')\n",
      "('pale', 'NN')\n",
      "('Usher', 'NNP')\n",
      "('--', ':')\n",
      "('threadbare', 'NN')\n"
     ]
    }
   ],
   "source": [
    "# Run: python -m nltk.downloader words maxent_ne_chunker gutenberg maxent_treebank_pos_tagger averaged_perceptron_tagger\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from itertools import islice\n",
    "\n",
    "moby_dick = gutenberg.raw('melville-moby_dick.txt')\n",
    "print('Moby Dick sample: {0}'.format(moby_dick[117:300]))\n",
    "\n",
    "words = word_tokenize(moby_dick[117:1000])\n",
    "\n",
    "word_pos = [(word, pos) for word, pos in pos_tag(words) ]\n",
    "\n",
    "for item in islice(word_pos, 5): # ou: for item in word_pos[:5]\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Données d'entraînement / Anglais\n",
    "![NLP](pictures/Module%204/NER-POS%20data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Données d'entraînement / Français\n",
    "![NLP](pictures/Module%204/POS%20-%20French.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Reconnaissance d'entités nommées (NER)\n",
    "\n",
    "![NLP](pictures/NER.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemple avec NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('Blobby', 'PERSON'), ('Jamie Rose', 'ORGANIZATION'), ('Motion Picture Arts', 'ORGANIZATION'), ('Blobby', 'ORGANIZATION'), ('Will Carling', 'PERSON'), ('Noel', 'PERSON'), ('Academy', 'ORGANIZATION'), ('Noel Edmonds', 'PERSON'), ('Oscars', 'PERSON'), ('House Party', 'ORGANIZATION'), ('Sciences', 'ORGANIZATION'), ('Wayne Sleep', 'PERSON')}\n"
     ]
    }
   ],
   "source": [
    "# Run: python -m nltk.downloader words maxent_ne_chunker gutenberg maxent_treebank_pos_tagger averaged_perceptron_tagger\n",
    "import nltk, re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "def extract_entity_names(t):\n",
    "    entity_names = []\n",
    "\n",
    "    if hasattr(t, 'label') and t.label:\n",
    "        if t.label() in ['ORGANIZATION','PERSON','GPE']:\n",
    "            entity_names.append((' '.join([child[0] for child in t]),t.label()))\n",
    "        else:\n",
    "            for child in t:\n",
    "                entity_names.extend(extract_entity_names(child))\n",
    "\n",
    "    return entity_names\n",
    "\n",
    "# Input data\n",
    "with open('data/NER Sample.txt', 'r') as f:\n",
    "    sample = f.read()\n",
    "\n",
    "# Séparation des phrases\n",
    "sentences = nltk.sent_tokenize(sample)\n",
    "\n",
    "# Séparation des mots\n",
    "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "# Étiquetage morpho-syntaxique\n",
    "tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]\n",
    "\n",
    "# Application de NER\n",
    "chunked_sentences = nltk.ne_chunk_sents(tagged_sentences, binary=False)\n",
    "\n",
    "entity_names = []\n",
    "for tree in chunked_sentences:\n",
    "    entity_names.extend(extract_entity_names(tree))\n",
    "\n",
    "# Print unique entity names\n",
    "print(set(entity_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Catégorisation des commentaires (Amazon)\n",
    "![NLP](pictures/Module%204/Amazon%20comments.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![NLP](pictures/Module%204/Amazon%20review%20-%20data.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
